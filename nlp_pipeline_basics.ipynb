{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "273joOAaVi1Q"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MVWNWRXWR10",
        "outputId": "81ab256e-35a8-4d2f-bd20-4a307b60c253"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a sample text\n",
        "text = \"Hello! My name is Random Joe, and I am a software engineer at Google in Miami. I have been working there for over five years, and I specialize in Natural Language Processing (NLP) and Machine Learning. In my free time, I enjoy hiking in the Adirondack Mountains, reading books about artificial intelligence, and exploring new coffee shops in Brooklyn. Last summer, I visited Paris, France, and it was an incredible experience. I also love collaborating with my colleagues, such as Dr. Jane Smith, who is a leading expert in computer vision. Together, we are working on a project to improve sentiment analysis algorithms using advanced NLP techniques. By the way, did you know that the Eiffel Tower was completed in 1889? It's one of the most iconic landmarks in the world!\""
      ],
      "metadata": {
        "id": "2HfNvYwrWZWa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization - Tokenization splits the text into individual sentences and words for further processing.\n",
        "# Function: Tokenize the text into sentences and words\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(\"Tokenization:\")\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmIUVlInWktt",
        "outputId": "0f7fd6c4-f59f-49af-ef26-22882b7e932c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:\n",
            "Sentences: ['Hello!', 'My name is Random Joe, and I am a software engineer at Google in Miami.', 'I have been working there for over five years, and I specialize in Natural Language Processing (NLP) and Machine Learning.', 'In my free time, I enjoy hiking in the Adirondack Mountains, reading books about artificial intelligence, and exploring new coffee shops in Brooklyn.', 'Last summer, I visited Paris, France, and it was an incredible experience.', 'I also love collaborating with my colleagues, such as Dr. Jane Smith, who is a leading expert in computer vision.', 'Together, we are working on a project to improve sentiment analysis algorithms using advanced NLP techniques.', 'By the way, did you know that the Eiffel Tower was completed in 1889?', \"It's one of the most iconic landmarks in the world!\"]\n",
            "Words: ['Hello', '!', 'My', 'name', 'is', 'Random', 'Joe', ',', 'and', 'I', 'am', 'a', 'software', 'engineer', 'at', 'Google', 'in', 'Miami', '.', 'I', 'have', 'been', 'working', 'there', 'for', 'over', 'five', 'years', ',', 'and', 'I', 'specialize', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'and', 'Machine', 'Learning', '.', 'In', 'my', 'free', 'time', ',', 'I', 'enjoy', 'hiking', 'in', 'the', 'Adirondack', 'Mountains', ',', 'reading', 'books', 'about', 'artificial', 'intelligence', ',', 'and', 'exploring', 'new', 'coffee', 'shops', 'in', 'Brooklyn', '.', 'Last', 'summer', ',', 'I', 'visited', 'Paris', ',', 'France', ',', 'and', 'it', 'was', 'an', 'incredible', 'experience', '.', 'I', 'also', 'love', 'collaborating', 'with', 'my', 'colleagues', ',', 'such', 'as', 'Dr.', 'Jane', 'Smith', ',', 'who', 'is', 'a', 'leading', 'expert', 'in', 'computer', 'vision', '.', 'Together', ',', 'we', 'are', 'working', 'on', 'a', 'project', 'to', 'improve', 'sentiment', 'analysis', 'algorithms', 'using', 'advanced', 'NLP', 'techniques', '.', 'By', 'the', 'way', ',', 'did', 'you', 'know', 'that', 'the', 'Eiffel', 'Tower', 'was', 'completed', 'in', '1889', '?', 'It', \"'s\", 'one', 'of', 'the', 'most', 'iconic', 'landmarks', 'in', 'the', 'world', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming - Stemming reduces words to their root form, which helps in normalizing text.\n",
        "# Function: Apply stemming to the word tokens using NLTK's PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"\\nStemming:\")\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwPf3ezgW2x6",
        "outputId": "4b6693bf-f054-453c-9b47-b78ec0157a74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming:\n",
            "Stemmed Words: ['hello', '!', 'my', 'name', 'is', 'random', 'joe', ',', 'and', 'i', 'am', 'a', 'softwar', 'engin', 'at', 'googl', 'in', 'miami', '.', 'i', 'have', 'been', 'work', 'there', 'for', 'over', 'five', 'year', ',', 'and', 'i', 'special', 'in', 'natur', 'languag', 'process', '(', 'nlp', ')', 'and', 'machin', 'learn', '.', 'in', 'my', 'free', 'time', ',', 'i', 'enjoy', 'hike', 'in', 'the', 'adirondack', 'mountain', ',', 'read', 'book', 'about', 'artifici', 'intellig', ',', 'and', 'explor', 'new', 'coffe', 'shop', 'in', 'brooklyn', '.', 'last', 'summer', ',', 'i', 'visit', 'pari', ',', 'franc', ',', 'and', 'it', 'wa', 'an', 'incred', 'experi', '.', 'i', 'also', 'love', 'collabor', 'with', 'my', 'colleagu', ',', 'such', 'as', 'dr.', 'jane', 'smith', ',', 'who', 'is', 'a', 'lead', 'expert', 'in', 'comput', 'vision', '.', 'togeth', ',', 'we', 'are', 'work', 'on', 'a', 'project', 'to', 'improv', 'sentiment', 'analysi', 'algorithm', 'use', 'advanc', 'nlp', 'techniqu', '.', 'by', 'the', 'way', ',', 'did', 'you', 'know', 'that', 'the', 'eiffel', 'tower', 'wa', 'complet', 'in', '1889', '?', 'it', \"'s\", 'one', 'of', 'the', 'most', 'icon', 'landmark', 'in', 'the', 'world', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization - Lemmatization reduces words to their base or dictionary form, providing more accurate normalization than stemming.\n",
        "# Function: Apply lemmatization to the word tokens using NLTK's WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"\\nLemmatization:\")\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXV0wbSzXH0o",
        "outputId": "9e9507bf-9bfa-4d41-fe1e-2cd8a6592df3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            "Lemmatized Words: ['Hello', '!', 'My', 'name', 'is', 'Random', 'Joe', ',', 'and', 'I', 'am', 'a', 'software', 'engineer', 'at', 'Google', 'in', 'Miami', '.', 'I', 'have', 'been', 'working', 'there', 'for', 'over', 'five', 'year', ',', 'and', 'I', 'specialize', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'and', 'Machine', 'Learning', '.', 'In', 'my', 'free', 'time', ',', 'I', 'enjoy', 'hiking', 'in', 'the', 'Adirondack', 'Mountains', ',', 'reading', 'book', 'about', 'artificial', 'intelligence', ',', 'and', 'exploring', 'new', 'coffee', 'shop', 'in', 'Brooklyn', '.', 'Last', 'summer', ',', 'I', 'visited', 'Paris', ',', 'France', ',', 'and', 'it', 'wa', 'an', 'incredible', 'experience', '.', 'I', 'also', 'love', 'collaborating', 'with', 'my', 'colleague', ',', 'such', 'a', 'Dr.', 'Jane', 'Smith', ',', 'who', 'is', 'a', 'leading', 'expert', 'in', 'computer', 'vision', '.', 'Together', ',', 'we', 'are', 'working', 'on', 'a', 'project', 'to', 'improve', 'sentiment', 'analysis', 'algorithm', 'using', 'advanced', 'NLP', 'technique', '.', 'By', 'the', 'way', ',', 'did', 'you', 'know', 'that', 'the', 'Eiffel', 'Tower', 'wa', 'completed', 'in', '1889', '?', 'It', \"'s\", 'one', 'of', 'the', 'most', 'iconic', 'landmark', 'in', 'the', 'world', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parts of Speech (POS) Tagging - POS tagging assigns grammatical labels (e.g., noun, verb) to each word in the text.\n",
        "# Function: Collect the parts of speech for each word in the text\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "print(\"\\nPOS Tagging:\")\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GBEHnugXyZo",
        "outputId": "d628e579-1dc9-4f47-9029-97dcd5c88d62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tagging:\n",
            "POS Tags: [('Hello', 'NN'), ('!', '.'), ('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Random', 'NNP'), ('Joe', 'NNP'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('software', 'NN'), ('engineer', 'NN'), ('at', 'IN'), ('Google', 'NNP'), ('in', 'IN'), ('Miami', 'NNP'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('been', 'VBN'), ('working', 'VBG'), ('there', 'RB'), ('for', 'IN'), ('over', 'IN'), ('five', 'CD'), ('years', 'NNS'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('specialize', 'VBP'), ('in', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('and', 'CC'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('.', '.'), ('In', 'IN'), ('my', 'PRP$'), ('free', 'JJ'), ('time', 'NN'), (',', ','), ('I', 'PRP'), ('enjoy', 'VBP'), ('hiking', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('Adirondack', 'NNP'), ('Mountains', 'NNP'), (',', ','), ('reading', 'VBG'), ('books', 'NNS'), ('about', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), (',', ','), ('and', 'CC'), ('exploring', 'VBG'), ('new', 'JJ'), ('coffee', 'NN'), ('shops', 'NNS'), ('in', 'IN'), ('Brooklyn', 'NNP'), ('.', '.'), ('Last', 'JJ'), ('summer', 'NN'), (',', ','), ('I', 'PRP'), ('visited', 'VBD'), ('Paris', 'NNP'), (',', ','), ('France', 'NNP'), (',', ','), ('and', 'CC'), ('it', 'PRP'), ('was', 'VBD'), ('an', 'DT'), ('incredible', 'JJ'), ('experience', 'NN'), ('.', '.'), ('I', 'PRP'), ('also', 'RB'), ('love', 'VBP'), ('collaborating', 'VBG'), ('with', 'IN'), ('my', 'PRP$'), ('colleagues', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('Dr.', 'NNP'), ('Jane', 'NNP'), ('Smith', 'NNP'), (',', ','), ('who', 'WP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('expert', 'NN'), ('in', 'IN'), ('computer', 'NN'), ('vision', 'NN'), ('.', '.'), ('Together', 'RB'), (',', ','), ('we', 'PRP'), ('are', 'VBP'), ('working', 'VBG'), ('on', 'IN'), ('a', 'DT'), ('project', 'NN'), ('to', 'TO'), ('improve', 'VB'), ('sentiment', 'NN'), ('analysis', 'NN'), ('algorithms', 'NN'), ('using', 'VBG'), ('advanced', 'JJ'), ('NLP', 'NNP'), ('techniques', 'NNS'), ('.', '.'), ('By', 'IN'), ('the', 'DT'), ('way', 'NN'), (',', ','), ('did', 'VBD'), ('you', 'PRP'), ('know', 'VBP'), ('that', 'IN'), ('the', 'DT'), ('Eiffel', 'NNP'), ('Tower', 'NNP'), ('was', 'VBD'), ('completed', 'VBN'), ('in', 'IN'), ('1889', 'CD'), ('?', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('iconic', 'JJ'), ('landmarks', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify Punctuation - Punctuation identification separates punctuation marks from the text for further processing.\n",
        "# Function: Identify punctuation marks from the tokenized words\n",
        "punctuation = [word for word in words if not word.isalnum()]\n",
        "\n",
        "print(\"\\nPunctuation Identification:\")\n",
        "print(\"Punctuation Marks:\", punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki-yEppfYU5n",
        "outputId": "044605e1-4e29-4b02-c010-2df6e955c11f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation Identification:\n",
            "Punctuation Marks: ['!', ',', '.', ',', '(', ')', '.', ',', ',', ',', '.', ',', ',', ',', '.', ',', 'Dr.', ',', '.', ',', '.', ',', '?', \"'s\", '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Words Removal - Eliminates common words (e.g., 'the', 'is') that do not contribute significantly to the meaning of the text.\n",
        "# Function: Identify and remove stop words from the word tokens\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"\\nStop Words Removal:\")\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6jfpRV4YqHH",
        "outputId": "53c35e9d-091b-4dab-fdbc-b5ab4c8fef9f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stop Words Removal:\n",
            "Filtered Words: ['Hello', '!', 'name', 'Random', 'Joe', ',', 'software', 'engineer', 'Google', 'Miami', '.', 'working', 'five', 'years', ',', 'specialize', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'Machine', 'Learning', '.', 'free', 'time', ',', 'enjoy', 'hiking', 'Adirondack', 'Mountains', ',', 'reading', 'books', 'artificial', 'intelligence', ',', 'exploring', 'new', 'coffee', 'shops', 'Brooklyn', '.', 'Last', 'summer', ',', 'visited', 'Paris', ',', 'France', ',', 'incredible', 'experience', '.', 'also', 'love', 'collaborating', 'colleagues', ',', 'Dr.', 'Jane', 'Smith', ',', 'leading', 'expert', 'computer', 'vision', '.', 'Together', ',', 'working', 'project', 'improve', 'sentiment', 'analysis', 'algorithms', 'using', 'advanced', 'NLP', 'techniques', '.', 'way', ',', 'know', 'Eiffel', 'Tower', 'completed', '1889', '?', \"'s\", 'one', 'iconic', 'landmarks', 'world', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the Data - Removes unnecessary elements like punctuation and stop words to prepare the text for analysis.\n",
        "# Function: Remove punctuation and stop words to create a clean version of the text\n",
        "clean_words = [word for word in filtered_words if word.isalnum()]\n",
        "\n",
        "print(\"\\nCleaned Data:\")\n",
        "print(\"Clean Words:\", clean_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJha6AR-ZNVe",
        "outputId": "f2f268b6-00ae-45d2-83f7-16c5bf10881a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaned Data:\n",
            "Clean Words: ['Hello', 'name', 'Random', 'Joe', 'software', 'engineer', 'Google', 'Miami', 'working', 'five', 'years', 'specialize', 'Natural', 'Language', 'Processing', 'NLP', 'Machine', 'Learning', 'free', 'time', 'enjoy', 'hiking', 'Adirondack', 'Mountains', 'reading', 'books', 'artificial', 'intelligence', 'exploring', 'new', 'coffee', 'shops', 'Brooklyn', 'Last', 'summer', 'visited', 'Paris', 'France', 'incredible', 'experience', 'also', 'love', 'collaborating', 'colleagues', 'Jane', 'Smith', 'leading', 'expert', 'computer', 'vision', 'Together', 'working', 'project', 'improve', 'sentiment', 'analysis', 'algorithms', 'using', 'advanced', 'NLP', 'techniques', 'way', 'know', 'Eiffel', 'Tower', 'completed', '1889', 'one', 'iconic', 'landmarks', 'world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition (NER) - Identifies and classifies named entities (e.g., people, locations) in the text.\n",
        "# Function: Perform Named Entity Recognition on the text\n",
        "ner_tags = ne_chunk(pos_tags)\n",
        "\n",
        "named_entities = []\n",
        "for chunk in ner_tags:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        entity = \" \".join(c[0] for c in chunk)\n",
        "        label = chunk.label()\n",
        "        named_entities.append((entity, label))\n",
        "\n",
        "# Create DF to display the named entities in better visual (table)\n",
        "ner_df = pd.DataFrame(named_entities, columns=[\"Entity\", \"Label\"])\n",
        "\n",
        "print(\"Named Entity Recognition (NER) Results:\")\n",
        "print(ner_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Li1ilHWZivz",
        "outputId": "82266248-45a1-4015-d45c-b6a333a63ac8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity Recognition (NER) Results:\n",
            "                  Entity         Label\n",
            "0                  Hello           GPE\n",
            "1             Random Joe        PERSON\n",
            "2                 Google  ORGANIZATION\n",
            "3                  Miami           GPE\n",
            "4       Natural Language  ORGANIZATION\n",
            "5                    NLP  ORGANIZATION\n",
            "6       Machine Learning        PERSON\n",
            "7   Adirondack Mountains  ORGANIZATION\n",
            "8               Brooklyn           GPE\n",
            "9                  Paris           GPE\n",
            "10                France           GPE\n",
            "11            Jane Smith        PERSON\n",
            "12                   NLP  ORGANIZATION\n",
            "13          Eiffel Tower  ORGANIZATION\n"
          ]
        }
      ]
    }
  ]
}